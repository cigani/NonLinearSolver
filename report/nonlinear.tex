\documentclass[11pt, a4paper]{article}
\usepackage{amsthm,amsmath,amssymb}
\usepackage{authblk}
\usepackage{gensymb}
\usepackage[
  colorlinks = true,
  linkcolor = blue,
  urlcolor = blue,
  citecolor = magenta,
]{hyperref}
\usepackage[
  backend = bibtex,
  maxbibnames = 99,
  sorting = none
]{biblatex}
\bibliography{references}
\ifx\pdfoutput\undefined
\usepackage{graphicx}
\else
\usepackage[pdftex]{graphicx}
\usepackage{epstopdf}
\fi
\usepackage{caption}
\usepackage{tikz} \usetikzlibrary{positioning}
\usepackage{booktabs}
\usepackage{float} %to use [H] for images
\usepackage[left=24mm,top=24mm,right=24mm,bottom=24mm]{geometry} %to adjust the margins
\newcommand{\nb}[1]{} %to write inline comments
\usepackage{mdframed}
\usepackage{lipsum}
\newmdtheoremenv{theo}{Theorem}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}


% Document starts
\begin{document}

\begingroup
\thispagestyle{empty}
\centering
\par\normalfont\fontsize{15}{20}\sffamily\selectfont
\textbf{MATH-458 Programming Concepts in Scientific Computing: Nonlinear Solvers}\\
\vspace*{0.4cm}
{\large{Michael Jaquier}}
\hspace*{0.5cm}
{\large{Alexander Lorkowski}}\par 
\endgroup

\section{Introduction}

% Head 1
\section{Algorithm}
\subsection{Bisection Method}
The bisection method is based on Theorem 1.
\begin{theo}
  (Zeros of a continuous functions)\\
  Given a continuous function f : [a,b] $\rightarrow$ R, such that f(a)f(b) $<$ 0, then $\exists$ at least an $\alpha$ $\subset$ (a,b) such that f($\alpha$) = 0.
\end{theo}

Starting from I$_0$ = [a,b], the bisection method generates a sequence of subintervals I$_k$ = [a(k),b(k)], k $\geq$ 0, with I$_k$ $\subset$ I$_k$−1, k $\geq$ 1, and enjoys the property that f(a(k))f(b(k)) $<$ 0.

\begin{algorithm}
  \caption{Bisection Method}\label{bisection}
  \begin{algorithmic}[1]
    \State  $\textit{k} \gets \text{0}$, $\textit{$a^{(0)}$} \gets \text{a}$, $\textit{$b^{(0)}$} \gets \text{b}$, $\textit{$x^{(0)}$} \gets \text{($a^{(0)}$+$b^{(0)}$)/2}$
    \While{$x^{(k)}$ $>$ tol and k $<$ $k_{max}$}
    \If {f($x^{(k-1)}$) == 0}
    \Return $\alpha$ = $x^{(k-1)}$
    \Else
    \If{f($x^{(k-1)}$)f($a^{(k-1)}$) $<$ 0}
    \State set $a^{(k)}$ = $a^{(k-1)}$ and $b^{(k)}$ = $x^{(k-1)}$
    \EndIf
    \If{f($x^{(k-1)}$)f($b^{(k-1)}$) $<$ 0}
    \State set $a^{(k)}$ = $x^{(k-1)}$ and $b^{(k)}$ = $b^{(k-1)}$
    \EndIf
    \State set $x^{(k)}$ = ($a^{(k)}$+$b^{(k)}$)/2
    \EndIf
    \EndWhile
    \Return $\alpha$ = $x^{(k)}$
  \end{algorithmic}
\end{algorithm}

\subsection{Newton/Quasi-Newton Methods}
Assuming that f $\subset$ $C^{0}(I)$ and is differentiable in the interval I = (a,b) $\subseteq$ R, the equation of a tangenet line to the curve ($x$, f($x$)) at coordinate $x^{(k)}$, where $x^{(k)}$ $\subset$ I, is y(x) = f($x^{(k)}$)+df($x^{(k)}$)(x - $x^{(k)}$).  If we assume y($x^{(k)}$) = 0, $x^{(k+1)}$ can be computed from Eq. \ref{eq:generalnewton}.

\begin{equation} \label{eq:generalnewton}
  x^{(k+1)} = x^{(k)} - \frac{f(x^{(k)})}{q^{(k)}}
\end{equation}

with $q^{(k)}$ determining the method as Newton (Eq. \ref{eq:newton}) or Chord (Eq. \ref{eq:chord}):

\begin{equation} \label{eq:newton}
  q^{(k)} = \frac{df(x^{(k)})}{dx}
\end{equation}

\begin{equation} \label{eq:chord}
  q^{(k)} = \frac{f(x^{(k)})-f(x^{(k-1)})}{x^{(k)}-x^{(k-1)}}
\end{equation}

\begin{algorithm}
  \caption{General quasi-Newton Method}\label{quasinewton}
  \begin{algorithmic}[1]
    \State  $\textit{k} \gets \text{0}$, set initial guess $x^{(0)}$  
    \While{$x^{(k)}$ $>$ tol and k $<$ $k_{max}$}
    \State $x^{(k+1)}$ = $x^{(k)}$ - $\frac{f(x^{(k)})}{(q^{(k)}}$
    \State k = k + 1;
    \EndWhile
    \Return $\alpha$ = $x^{(k)}$
  \end{algorithmic}
\end{algorithm}

\subsection{Fixed Point Iteration}
For a given f : [a,b] $\rightarrow$ R, the problem can be transformed from f(x) = 0 into an equivalent problem x - $\phi$(x) = 0.  The auxiliary function $\phi$ : [a,b] $\rightarrow$ R has to chosen in such a way that $\phi$($\alpha$) = $\alpha$ whenever f($\alpha$) = 0. Finding the fixed points of the mapping $\phi$ thus results in finding the roots of the original equation f(x), hence the name fixed point iteration.

\begin{algorithm}
  \caption{Fixed point iterations}\label{fixedpoint}
  \begin{algorithmic}[1]
    \State  $\textit{k} \gets \text{0}$, set initial guess $x^{(0)}$  
    \While{$x^{(k)}$ - $x^{(k-1)}$ $>$ tol and k $<$ $k_{max}$}
    \State $x^{(k+1)}$ = $\phi(x^{(k)})$
    \State k = k + 1;
    \EndWhile
    \Return $\alpha$ = $x^{(k)}$
  \end{algorithmic}
\end{algorithm}

\subsection{Aitken Method}
Aitken’s method provides a way to accelerate the convergence of iterative methods for finding the roots of a function.  The general algorithm is shown below.

\begin{algorithm}
  \caption{Aitken Method}\label{fixedpoint}
  \begin{algorithmic}[1]
    \State  $\textit{k} \gets \text{0}$, set initial guess $x^{(0)}$  
    \While{$x^{(k)}$ - $x^{(k-1)}$ $>$ tol and k $<$ $k_{max}$}
    \State Calculate $x_{3i+1}$ and $x_{3i+2}$ using any linear iterative method.
    \State Modify $x_{3i+2}$ using $x_{3i+2}$ = $x_{3i}$ - $\frac{(x_{3i+1}-x_{3i})^{2}}{(x_{3i}-2*x_{3i+1}+x_{2i+2})}$
    \State k = k + 1;
    \EndWhile
    \Return $\alpha$ = $x^{(k)}$
  \end{algorithmic}
\end{algorithm}

\section{Program Structure}
\end{document}
